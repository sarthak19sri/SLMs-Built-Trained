1) For Gemma3:
torch>=2.1.0
tiktoken>=0.4.0
numpy>=1.24.0
tqdm>=4.65.0
matplotlib>=3.7.0
datasets>=2.14.0

2) For fine-tuning vs LoRA on BERT-mini Training
transformers>=4.35.0
datasets>=2.14.0
scikit-learn>=1.3.0
peft>=0.4.0
huggingface-hub>=0.17.0
torch>=2.1.0
numpy>=1.24.0
