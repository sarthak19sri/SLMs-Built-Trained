1) Built and pre-trained a 270M parameter Transformer (Gemma 3) end-to-end, including dataset processing with byte pair encoding (BPE) tokenization, input-output pair creation for next-token prediction, model architecture assembly, and efficient training on a single A6000 GPU, demonstrating coherent text generation capabilities.

2) Fine-tuned BERT-mini on the Yelp Review Full dataset (~650K reviews) for 3-class sentiment classification (Bad, Good, Excellent). Explored two strategies: full fine-tuning (~11M parameters) and LoRA (~0.1M parameters, ~1% of full) for parameter-efficient adaptation. Preprocessing involved tokenization, truncation/padding, and generating input IDs with attention masks. Training was conducted for 5 epochs with AdamW, cross-entropy loss, and evaluation using accuracy and F1-score. Full fine-tuning achieved 72.1% accuracy and 71.6% F1, outperforming LoRA (60.4% accuracy, 53.7% F1) while LoRA demonstrated significant parameter efficiency. This highlights the trade-off between performance and efficiency in small transformer models.